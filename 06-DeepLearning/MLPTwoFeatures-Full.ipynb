{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron (MLP)\n",
    "In questo notebook addestriamo un modello MLP con dati sintetici per un problema di classificazione binaria. Dato un dataset etichettato di dati sintetici, addestrizmo un modello MLP per trovare un confine decisionale tra due classi NON linearmente separabili.\n",
    "Abbiamo provato ad eseguire lo stesso task con Logistic Regression (laboratorio [Logistic Regression](../05-LogisticRegression/)). Ma per riuscirci abbiamo dovuto ricorrere a features polinomiali.\n",
    "\n",
    "Il vantaggio delle reti neurali come MLP e' che possono risolvere problemi complessi richiedendo una preparazione meno laboriosa dei dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Roberto Doriguzzi-Corin\n",
    "# Project: Corso di Algoritmi di Machine Learning per la rilevazione di attacchi informatici\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Import necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import classification_report, f1_score,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "import time\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "# Create a synthetic dataset with two classes that are not linearly separable\n",
    "X, y = make_circles(n_samples=1000, noise=0.1, factor=0.3, random_state=SEED)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Dataset sintetico con due classi di punti NON linearmente separabili')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementazione del modello\n",
    "Nella prossima cella, partiamo da un modello di Logistic Regression (cioe' una rete neurale MLP senza hidden layers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression model\n",
    "model = Sequential()\n",
    "\n",
    "# Questo e' l'input layer\n",
    "model.add(Input(shape=(2,)))\n",
    "\n",
    "# Questo e' l'output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "Vediamo se riusciamo a separare le due classi con un modello di Logistic Regrassion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analizziamo il processo di training\n",
    "Analizziamo il grafico dell'errore sul training e validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = model.history.history\n",
    "plt.plot(history_dict['loss'], label='Errore sul training set (Training Loss)')\n",
    "plt.plot(history_dict.get('val_loss', []), label='Errore sul validation set (Validation Loss)', linestyle='dashed')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Errore (Loss)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizziamo il decision boundary sul training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, bias = model.layers[0].get_weights()\n",
    "coefficients = [weights[0][0], weights[1][0], bias[0]]\n",
    "\n",
    "# Calculate slope and intercept for the decision boundary line\n",
    "slope = -coefficients[0] / coefficients[1]\n",
    "intercept = -coefficients[2] / coefficients[1]\n",
    "\n",
    "# Plot the data points and decision boundary line\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', cmap='viridis')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Plot the decision boundary line\n",
    "x_min, x_max = X_train[:, 0].min(), X_train[:, 0].max()\n",
    "y_min, y_max = X_train[:, 1].min(), X_train[:, 1].max()\n",
    "plt.plot([x_min, x_max], [-1, 1], color='red', linestyle='--')\n",
    "\n",
    "plt.title('Decision Boundary con una linea retta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usiamo il modello addestrato sul test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,test_accuracy = model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trasformiamo Logistic Regression in un modello MLP\n",
    "Proviamo ora a risolvere il problema NON linearmente separabile con una rete neurale di tipo MLP. \n",
    "Per fare cio', modifichiamo il codice di Logistic Regression aggiungendo uno o piu' hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model\n",
    "def create_model(hidden_layers=0, hidden_units=1):\n",
    "    model = Sequential(name  = \"mlp\")\n",
    "    # Questo e' l'input layer\n",
    "    model.add(Input(shape=(2,)))\n",
    "    \n",
    "    # I seguenti sono gli hidden layers\n",
    "    for layer in range(hidden_layers):\n",
    "        model.add(Dense(hidden_units, activation='relu'))\n",
    "    \n",
    "    # Infine l'output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    print (model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuriamo manualmente il modello MLP\n",
    "Scegliamo manualmente il numero di hidden layers e hidden units (neuroni) con cui configurare il modello. Scegliamo anche per quante epoche addestrare il modello assegnando un valore alla variabile ```EPOCHS```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cambia questi valori manualmente ####\n",
    "HIDDEN_LAYERS= 1\n",
    "HIDDEN_UNITS = 4\n",
    "EPOCHS=100\n",
    "#########################################\n",
    "\n",
    "model = create_model(hidden_layers=HIDDEN_LAYERS,hidden_units=HIDDEN_UNITS)\n",
    "\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_val, y_val))\n",
    "stop_time = time.time()\n",
    "\n",
    "# Total training time\n",
    "print(\"Total training time (sec): \", stop_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analizziamo il processo di training\n",
    "Cosa possiamo cambiare nel modello (o nel numero di epoche) per ottenere un modello ottimale. Analizziamo il grafico dell'errore sul training e validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = model.history.history\n",
    "plt.plot(history_dict['loss'], label='Errore sul training set (Training Loss)')\n",
    "plt.plot(history_dict.get('val_loss', []), label='Errore sul validation set (Validation Loss)', linestyle='dashed')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Errore (Loss)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usiamo il modello addestrato sul test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,test_accuracy = model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizziamo il decision boundary rispetto al training set\n",
    "Vediamo quanto ha imparato il modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary as a single curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "h = .02  # Step size in the mesh\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "grid = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "# Predict probabilities for each point on the meshgrid\n",
    "Z = model.predict(grid)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the contour line representing the decision boundary (where probability is 0.5)\n",
    "plt.contour(xx, yy, Z, levels=[0.5], colors='black')\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', cmap='viridis')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Boundary con MLP')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early stopping\n",
    "Invece di inserire il numero di epoche a mano, possiamo fare qualcosa di piu' furbo? **Early Stopping** e' un metodo per addestrare un modello per un numero ottimale di epoche.\n",
    "Anche in questo caso va assegnato un parametro, chiamato ```PATIENCE``` (pazienza), che indica all'algoritmo quante epoche senza milgioramenti aspettare prima di fermare l'addestramento. Ad esempio, con ```PATIENCE=10```, se l'errore (loss) sul validation set raggiunge ```0.1``` e nelle successive 10 epoche non decresce, il processo viene fermato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cambia questi valori manualmente ###\n",
    "HIDDEN_LAYERS= 1\n",
    "HIDDEN_UNITS = 4\n",
    "\n",
    "PATIENCE = 20\n",
    "########################################\n",
    "\n",
    "\n",
    "model = create_model(hidden_layers=HIDDEN_LAYERS,hidden_units=HIDDEN_UNITS)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE, restore_best_weights=True)\n",
    "\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), callbacks= [early_stopping])\n",
    "stop_time = time.time()\n",
    "\n",
    "# Total training time\n",
    "print(\"Total training time (sec): \", stop_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analizziamo il processo di training\n",
    "Analizziamo di nuovo il grafico dell'errore sul training e validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = model.history.history\n",
    "plt.plot(history_dict['loss'], label='Errore sul training set (Training Loss)')\n",
    "plt.plot(history_dict.get('val_loss', []), label='Errore sul validation set (Validation Loss)', linestyle='dashed')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Errore (Loss)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usiamo il modello addestrato sul test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,test_accuracy = model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizziamo il decision boundary rispetto al training set\n",
    "Vediamo quanto ha imparato il modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary as a single curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "h = .02  # Step size in the mesh\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "grid = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "# Predict probabilities for each point on the meshgrid\n",
    "Z = model.predict(grid)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the contour line representing the decision boundary (where probability is 0.5)\n",
    "plt.contour(xx, yy, Z, levels=[0.5], colors='black')\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', cmap='viridis')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Boundary con MLP')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search\n",
    "Il codice nella cella seguente esegue la configurazione automatica del modello MLP con la strategia ```grid search```.\n",
    "Grid Search è una tecnica di ottimizzazione dei parametri dei modelli di Deep Learning (ma anche di modelli di Machine Learning) che consiste nella ricerca sistematica della combinazione ottimale di parametri (per esempio, hidden layers e hidden units (neuroni)), valutando le prestazioni di un modello su una griglia di possibili valori di questi parametri. La griglia dei parametri è  definita dal programmatore. \n",
    "\n",
    "Il tuo compito e' di inserire alcuni valori nella griglia sotto e di far partire l'addestramento del modello MLP.\n",
    "Assegna anche un valore di ```PATIENCE``` assegnando un numero intero alla variabile ```PATIENCE```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KerasClassifier based on the create_model function\n",
    "model = KerasClassifier(build_fn=create_model, batch_size=100, verbose=1)\n",
    "\n",
    "### Inserisci alcuni valori interi separati da virgola tra le parentesi quadre ####\n",
    "param_grid = {\n",
    "    'hidden_layers' : [1,2],\n",
    "    'hidden_units' : [4,8]\n",
    "}\n",
    "###########################################################\n",
    "\n",
    "### Stabilisci il valore di PATIENCE (per esempio 10) ###\n",
    "PATIENCE = 20\n",
    "###########################################################\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=2)\n",
    "\n",
    "### Add early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE, restore_best_weights=True)\n",
    "\n",
    "start_time = time.time()\n",
    "grid_result = grid.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), callbacks= [early_stopping])\n",
    "stop_time = time.time()\n",
    "\n",
    "# Total training time\n",
    "print(\"Total training time (sec): \", stop_time-start_time)\n",
    "# Print the best parameters and corresponding accuracy\n",
    "print(\"Best parameters found: \", grid_result.best_params_)\n",
    "print(\"Best cross-validated accuracy: {:.2f}\".format(grid_result.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision boundary\n",
    "Vediamo se il nostro MLP e' in grado di distinguere le due classi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary as a single curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "h = .02  # Step size in the mesh\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "grid = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "# Predict probabilities for each point on the meshgrid\n",
    "Z = best_model.predict(grid)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the contour line representing the decision boundary (where probability is 0.5)\n",
    "plt.contour(xx, yy, Z, levels=[0.5], colors='black')\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', cmap='viridis')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Boundary con MLP')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usiamo il modello sul test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.squeeze(best_model.predict(X_test, batch_size=32) > 0.5)\n",
    "\n",
    "print(\"F1 Score: \", f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizziamo se e dove sbaglia sul test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary as a single curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "h = .02  # Step size in the mesh\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "grid = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "# Predict probabilities for each point on the meshgrid\n",
    "Z = best_model.predict(grid)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the contour line representing the decision boundary (where probability is 0.5)\n",
    "plt.contour(xx, yy, Z, levels=[0.5], colors='black')\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolors='k', cmap='viridis')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Boundary con MLP')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
