{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression \n",
    "In questo notebook addrestriamo un modello di Logistic Regression con dati sintetici (dati generati a scopo dimostrativo) per un problema di classificazione binaria. Dato un dataset etichettato di dati sintetici, addestriamo un modello di Logistic Regression per trovare un confine decisionale (o ```decision boundary```) lineare tra due classi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Roberto Doriguzzi-Corin\n",
    "# Project: Corso di Algoritmi di Machine Learning per la rilevazione di attacchi informatici\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Import necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "# Create a synthetic dataset with two classes that are linearly separable\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_classes=2, n_informative=2, n_redundant=0,\n",
    "                           n_clusters_per_class=1, class_sep=2.0, random_state=SEED)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizziamo i campioni del dataset nel piano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', edgecolors='k')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Dataset sintetico con due classi di punti linearmente separabili')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementazione del modello\n",
    "Nella prossima cella, implementiamo il modello di Logistic Regression. \n",
    "Il modello e' creato con una procedura identica a quella usata per creare una rete neurale. L'unica differeza sta nella complessita' del modello. Nel caso di Logistic Regression basta una riga di codice (riga 3). Nel caso di una rete neurale profonda, no. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression model\n",
    "def create_model(input_shape):\n",
    "    model = Sequential(name  = \"logistic_regression\")\n",
    "    # Questo e' l'input layer. In questo caso abbiamo 21 features\n",
    "    model.add(Input(shape=input_shape))\n",
    "    \n",
    "    # Infine l'output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    print (model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "Nella cella seguente addestriamo il modello di Logistic Regression. L'output del processo di training mostra come l'accuracy del modello tenda a crescere ad ogni iterazione (```epoch```). Questo vuol dire che il modello sta imparando a distiguere i flussi di traffico benigni da quelli malevoli.\n",
    "Lo addestriamo per 100 epoche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = create_model(X_train.shape[1])\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizziamo il decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the learned coefficients (weights and bias) from the trained model\n",
    "weights, bias = model.layers[0].get_weights()\n",
    "coefficients = [weights[0][0], weights[1][0], bias[0]]\n",
    "\n",
    "# Print the learned coefficients\n",
    "print(\"Decision boundary: \" +  str(bias[0]) + \" + \" + str(weights[0][0]) + \"*X1\" + \" + \" + str(weights[1][0]) + \"*X2\")\n",
    "\n",
    "# Calculate slope and intercept for the decision boundary line\n",
    "slope = -coefficients[0] / coefficients[1]\n",
    "intercept = -coefficients[2] / coefficients[1]\n",
    "\n",
    "# Plot the data points and decision boundary line\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', cmap='viridis')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Plot the decision boundary line\n",
    "x_min, x_max = X_train[:, 0].min(), X_train[:, 0].max()\n",
    "y_min, y_max = X_train[:, 1].min(), X_train[:, 1].max()\n",
    "plt.plot([x_min, x_max], [slope * x_min + intercept, slope * x_max + intercept], color='red', linestyle='--')\n",
    "\n",
    "plt.title('Decision Boundary con una linea retta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate points for plotting the hyperplane in 3D space\n",
    "xx, yy = np.meshgrid(np.linspace(X[:, 0].min(), X[:, 0].max(), 50),\n",
    "                     np.linspace(X[:, 1].min(), X[:, 1].max(), 50))\n",
    "zz = (-coefficients[0] * xx - coefficients[1] * yy - coefficients[2]) / coefficients[2]\n",
    "\n",
    "\n",
    "# Plot the hyperplane in 3D space\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(xx, yy, zz, color='c', alpha=0.5)\n",
    "ax.scatter(X[:, 0], X[:, 1], y, c=y, edgecolors='k', cmap=plt.cm.Set1)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "ax.set_zlabel('Output (before the sigmoid function)')\n",
    "ax.set_title('Hyperplane in 3D Space')\n",
    "\n",
    "ax.view_init(elev=5, azim=180)  # Elev: Elevation (up/down), Azim: Azimuthal (rotation around the z axis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usiamo il modello con dei dati mai visti (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.squeeze(model.predict(X_test, batch_size=32) > 0.5)\n",
    "\n",
    "print(\"F1 Score: \", f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the learned coefficients (weights and bias) from the trained model\n",
    "weights, bias = model.layers[0].get_weights()\n",
    "coefficients = [weights[0][0], weights[1][0], bias[0]]\n",
    "\n",
    "# Print the learned coefficients\n",
    "print(\"Decision boundary: \" +  str(bias[0]) + \" + \" + str(weights[0][0]) + \"*X1\" + \" + \" + str(weights[1][0]) + \"*X2\")\n",
    "\n",
    "# Calculate slope and intercept for the decision boundary line\n",
    "slope = -coefficients[0] / coefficients[1]\n",
    "intercept = -coefficients[2] / coefficients[1]\n",
    "\n",
    "# Plot the data points and decision boundary line\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolors='k', cmap='viridis')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Plot the decision boundary line\n",
    "x_min, x_max = X_test[:, 0].min(), X_test[:, 0].max()\n",
    "y_min, y_max = X_test[:, 1].min(), X_test[:, 1].max()\n",
    "plt.plot([x_min, x_max], [slope * x_min + intercept, slope * x_max + intercept], color='red', linestyle='--')\n",
    "\n",
    "plt.title('Decision Boundary con una linea retta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression con features polinomiali\n",
    "La Logistic Regression può essere estesa per gestire caratteristiche polinomiali incorporando termini polinomiali nello spazio delle caratteristiche. Questa tecnica è nota come Logistic Regression Polinomiale. Permette al modello di Logistic Regression di catturare le relazioni non lineari tra le caratteristiche e la variabile target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "SEED  = 1\n",
    "\n",
    "GRADO_DEL_POLINOMIO=2\n",
    "\n",
    "# Create a synthetic dataset with two classes that are not linearly separable\n",
    "X, y = make_circles(n_samples=1000, noise=0.1, factor=0.3, random_state=SEED)\n",
    "\n",
    "# Add polynomial features to the data\n",
    "poly = PolynomialFeatures(degree=GRADO_DEL_POLINOMIO)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_poly_train, X_poly_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=SEED)\n",
    "X_poly_train, X_poly_val, y_train, y_val = train_test_split(X_poly_train, y_train, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Dataset sintetico con due classi di punti NON linearmente separabili')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = create_model(X_poly.shape[1])\n",
    "model.fit(X_poly_train, y_train, epochs=100, batch_size=32, validation_data=(X_poly_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary as a single curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "h = .02  # Step size in the mesh\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Transform meshgrid points into polynomial features\n",
    "xx_poly = poly.transform(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Predict probabilities for each point on the meshgrid\n",
    "Z = model.predict(xx_poly)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the contour line representing the decision boundary (where probability is 0.5)\n",
    "plt.contour(xx, yy, Z, levels=[0.5], colors='black')\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap='viridis')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Boundary con Features polinomiali')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usiamo il modello sul test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.squeeze(model.predict(X_poly_test, batch_size=32) > 0.5)\n",
    "\n",
    "print(\"F1 Score: \", f1_score(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
