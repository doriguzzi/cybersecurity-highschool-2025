{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection con Autoencoders\n",
    "In questo laboratorio, implementeremo e configureremo un Autoencoder per il rilevamento di anomalie di rete. L’autoencoder sara' addestrato e validato utilizzando campioni di traffico benigno e successivamente testato su anomalie di rete (in questo caso, attacchi informatici).\n",
    "Per trovare la miglior configurazione dell'Autoencoder, utilizzeremo la tecnica ```Grid Search```.\n",
    "\n",
    "| <img src=\"./autoencoder.png\" width=\"80%\">  |\n",
    "|--|\n",
    "| Architecture dell'Autoencoder|\n",
    "\n",
    "Addestreremo l'Autoencoder con traffico di rete benigno dal dataset CIC-DDoS2019 dell’Università del New Brunswick. Il traffico di rete è stato precedentemente pre-elaborato in modo che i pacchetti siano raggruppati in flussi di traffico bidirezionali utilizzando la 5-tupla (IP sorgente, IP destinazione, porta sorgente, porta destinazione, protocollo). Ogni flusso è rappresentato da 21 features (attributi) dell’header dei pacchetti calcolate da un massimo di 1000 pacchetti:\n",
    "\n",
    "| Feature nr.         | Feature Name |\n",
    "|---------------------|---------------------|\n",
    "| 00 | timestamp (mean IAT) | \n",
    "| 01 | packet_length (mean)| \n",
    "| 02 | IP_flags_df (sum) |\n",
    "| 03 | IP_flags_mf (sum) |\n",
    "| 04 | IP_flags_rb (sum) | \n",
    "| 05 | IP_frag_off (sum) |\n",
    "| 06 | protocols (mean) |\n",
    "| 07 | TCP_length (mean) |\n",
    "| 08 | TCP_flags_ack (sum) |\n",
    "| 09 | TCP_flags_cwr (sum) |\n",
    "| 10 | TCP_flags_ece (sum) |\n",
    "| 11 | TCP_flags_fin (sum) |\n",
    "| 12 | TCP_flags_push (sum) |\n",
    "| 13 | TCP_flags_res (sum) |\n",
    "| 14 | TCP_flags_reset (sum) |\n",
    "| 15 | TCP_flags_syn (sum) |\n",
    "| 16 | TCP_flags_urg (sum) |\n",
    "| 17 | TCP_window_size (mean) |\n",
    "| 18 | UDP_length (mean) |\n",
    "| 19 | ICMP_type (mean) |\n",
    "| 20 | Packets (counter)|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Roberto Doriguzzi-Corin\n",
    "# Project: Corso di Algoritmi di Machine Learning per la rilevazione di attacchi informatici\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import uniform, randint\n",
    "from util_functions import *\n",
    "DATASET_FOLDER = \"./DOS2019_Anomaly_Flatten\"\n",
    "X_train, _ = load_dataset(DATASET_FOLDER + \"/*\" + '-benign-train.hdf5')\n",
    "X_val, _ = load_dataset(DATASET_FOLDER + \"/*\" + '-benign-val.hdf5')\n",
    "X_test, _ = load_dataset(DATASET_FOLDER + \"/*\" + '-benign-test.hdf5')\n",
    "X_test_anomalies, _ = load_dataset(DATASET_FOLDER + \"/*\" + '-anomaly-test.hdf5')\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definizione del modello di Autoencoder\n",
    "Nella cella successiva, implementiamo l'Autoencoder definendo sia l'*encoder* che il *decoder*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hidden_units=10,coding_layer_size=2, learning_rate = 0.001, optimizer=Adam):\n",
    "    stacked_encoder = Sequential(name='Encoder',layers=[Input(shape=(X_train.shape[1],)), \n",
    "                              ### ADD YOUR CODE HERE ###\n",
    "                              Dense(hidden_units, activation=\"relu\"), \n",
    "                              Dense(coding_layer_size, activation=\"relu\")\n",
    "                              ##########################\n",
    "                              ]) \n",
    "\n",
    "    stacked_decoder = Sequential(name='Decoder',layers=[ \n",
    "                            ### ADD YOUR CODE HERE ###\n",
    "                            Dense(hidden_units, activation=\"relu\", input_shape=[coding_layer_size]), \n",
    "                            Dense(X_train.shape[1], activation=\"sigmoid\") \n",
    "                            ##########################\n",
    "                            ]) \n",
    "    stacked_ae = Sequential([stacked_encoder, stacked_decoder])\n",
    "\n",
    "    # Compile the model\n",
    "    stacked_ae.compile(optimizer=optimizer(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    print (stacked_encoder.summary())\n",
    "    print (stacked_decoder.summary())\n",
    "    return stacked_ae\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addrestramento dell'Autoencoder\n",
    "L'autoencoder sara' configurato cercando il numero ottimale di neuroni per la rappresentazione compressa dei flussi di rete benigni (```coding layer```) ed il numero di neuroni negli altri layers (```hidden_units```).\n",
    "Stabilisci un valore di ```PATIENCE``` ed inserisci delle liste di numeri interi separati da virgole tra le parentesi quadre nella cella sotto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KerasClassifier based on the create_model function\n",
    "stacked_ae = KerasRegressor(build_fn=create_model, batch_size=128, verbose=1)\n",
    "\n",
    "PATIENCE = \n",
    "\n",
    "param_dist = {\n",
    "    'hidden_units': [],\n",
    "    'coding_layer_size': []\n",
    "    ##########################\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator=stacked_ae, param_grid=param_dist, cv=2)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE, restore_best_weights=True)\n",
    "# Train the stacked autoencoder\n",
    "search_result = grid_search.fit(X_train, X_train,epochs=100, validation_data=(X_val, X_val),callbacks= [ early_stopping])\n",
    "\n",
    "# Print the best parameters and corresponding accuracy\n",
    "print(\"Configurazione migliore: \", search_result.best_params_)\n",
    "\n",
    "# Save the best model for later\n",
    "best_model = search_result.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test dell Autoencoder\n",
    "In quest’ultimo passaggio, valutiamo l’autoencoder su dati non visti. In particolare, testiamo la capacità del modello di rilevare anomalie di rete e la sua sensibilità agli outlier benigni misurando il Tasso di **Falsi Positivi**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the anomaly threshold using the erron on the validation data\n",
    "reconstructed_benign_validation = best_model.predict(X_val)\n",
    "reconstruction_error_benign_validation = np.mean(np.square(X_val - reconstructed_benign_validation), axis=1)\n",
    "# Set a threshold for anomaly detection (adjust as needed)\n",
    "anomaly_threshold = np.mean(reconstruction_error_benign_validation) + np.std(reconstruction_error_benign_validation)\n",
    "\n",
    "# Evaluate the model on unseen benign and anomalous traffic\n",
    "reconstructed_benign_test = best_model.predict(X_test)\n",
    "reconstructed_anomalies = best_model.predict(X_test_anomalies)\n",
    "\n",
    "# Calculate reconstruction errors on unseen data\n",
    "reconstruction_error_benign_test = np.mean(np.square(X_test - reconstructed_benign_test), axis=1)\n",
    "reconstruction_error_anomalies = np.mean(np.square(X_test_anomalies - reconstructed_anomalies), axis=1)\n",
    "\n",
    "# Identify anomalies\n",
    "false_positives = np.where(reconstruction_error_benign_test > anomaly_threshold)[0]\n",
    "anomalies = np.where(reconstruction_error_anomalies > anomaly_threshold)[0]\n",
    "\n",
    "# Print the indices of detected anomalies\n",
    "print(\"Detected anomalies:\", anomalies)\n",
    "print(\"Anomaly detection accuracy: \", float(len(anomalies))/X_test_anomalies.shape[0])\n",
    "print(\"False positive rate: \", float(len(false_positives))/X_test.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
